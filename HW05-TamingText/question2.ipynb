{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import show\n",
    "%matplotlib inline\n",
    "\n",
    "import glob # to find all files in folder\n",
    "\n",
    "import pycountry\n",
    "import re # regex\n",
    "from nltk.sentiment.util import *\n",
    "import nltk as nl\n",
    "from nltk.corpus import stopwords as nlstopw\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = 'hillary-clinton-emails/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all file in hillary-clinton-emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glob.glob(folder + '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aliases = pd.read_csv(folder + 'Aliases.csv')\n",
    "aliases.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "receivers = pd.read_csv(folder + 'EmailReceivers.csv')\n",
    "receivers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emails = pd.read_csv(folder + 'Emails.csv')\n",
    "emails.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "persons = pd.read_csv(folder + 'Persons.csv')\n",
    "persons.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns ExtractedBodyText is supposed to be the content of the mail but some of the mail have a ExtractedBodyText = NaN but the Rawtext seems to contains something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of emails: ', len(emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bodyNaN = emails.ExtractedBodyText.isnull().sum()\n",
    "print('Number of emails with ExtractedBodyText=NaN: {}, ({:.2f}%)'.format(emails.ExtractedBodyText.isnull().sum(), bodyNaN/ len(emails)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use the subject since it is usually a summary of the mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bodyNaN = emails.ExtractedSubject.isnull().sum()\n",
    "print('Number of emails with ExtractedSubject=NaN: {}, ({:.2f}%)'.format(emails.ExtractedBodyText.isnull().sum(), bodyNaN/ len(emails)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to combine the subject and the body and drop the mail that have both subject= NaN and body = Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subBodyNan = emails[np.logical_and(emails.ExtractedBodyText.isnull(),emails.ExtractedSubject.isnull())]\n",
    "print('Number of email where both subject and body is NaN: {}({:.2f})'.format(len(subBodyNan), len(subBodyNan)/ len(emails)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that number is small enough to drop all email where both Extracted subject and Extracted body is NaN.\n",
    "\n",
    "Let's drop them and create a new columns subjectBody that is the concatenation of the 2 columns ExtractedSubject and ExtractedBody. From now we will work with that columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails = emails[~ np.logical_and(emails.ExtractedBodyText.isnull(), emails.ExtractedSubject.isnull())]\n",
    "len(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails.ExtractedBodyText.fillna('',inplace=True)\n",
    "emails.ExtractedSubject.fillna('',inplace=True)\n",
    "emails['SubjectBody'] = emails.ExtractedBodyText + emails.ExtractedSubject\n",
    "emails.SubjectBody.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last check to be sur that our columns of interest don't have anymore NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of NaN in columns SubjectBody: ' ,emails.SubjectBody.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keep only mail that mentions a country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of a country in pycountry.countres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(pycountry.countries)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a dataframe with one line by countries and we count for each countries its occurences in the mail.\n",
    "\n",
    "Since a country can be reference in many way (Switzerland, switzerland, CH), we need to consider all the possible form. \n",
    "\n",
    "We may have a problem with word that have many meaning like US(country) and us (pronoun) so we can't just take all the country name in loer case and all the mail in lower case and just compare.\n",
    "\n",
    "Here are the consideration we use:\n",
    "    1. the country name can appear either in lower case, with the first letter in uppercase or all in uppercase\n",
    "    2. alpha_2 and alpha_3 are always used in uppercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_name = np.array([[country.name.lower(), country.name.upper(), country.name.title()] for country in list(pycountry.countries)])\n",
    "country_name[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_2 = np.array([country.alpha_2 for country in list(pycountry.countries)])\n",
    "alpha_2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_3 = np.array([country.alpha_3 for country in list(pycountry.countries)])\n",
    "alpha_3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countries = np.vstack((alpha_2, alpha_3)).T\n",
    "countries = np.concatenate([country_name, countries], axis=1)\n",
    "countries = pd.DataFrame(countries, columns=['name', 'NAME', 'Name', 'Alhpa_2', 'Alph_3'])\n",
    "countries.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countries.isin(['aruba']).any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_country(row):\n",
    "    return countries.isin(row.SubjectBody.split()).any().any()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails_country = emails[emails.apply(check_country, axis=1)]\n",
    "len(emails_country)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiments = pd.DataFrame(emails_country.SubjectBody)\n",
    "sentiments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the sentiment annalysis we need first to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the same pipeline as in exercice 1 to clean the emails:\n",
    "    - cleaning, tokenization, stopword removal, stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to remove ponctuation of each email\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove punktuation\n",
    "def removePunctuation(row):\n",
    "    return re.sub('['+string.punctuation+']', '', row.SubjectBody)\n",
    "\n",
    "sentiments['Punctuation']= sentiments.apply(removePunctuation, axis=1)\n",
    "sentiments.Punctuation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nl.download('punkt') # needed for word tokenization\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiments['Tokens'] = sentiments.Punctuation.apply(nl.word_tokenize)\n",
    "sentiments.Tokens.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to include also the stop word specific to the subject field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "email_stopwords = set(['re', 'fw', 'fvv', 'fwd']).union(stop)\n",
    "\n",
    "def removeStopWords(row):\n",
    "    return[i for i in row if i.lower() not in email_stopwords]\n",
    "\n",
    "sentiments['StopWords'] = sentiments.Tokens.apply(removeStopWords)\n",
    "sentiments.StopWords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmatize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer_E = EnglishStemmer()\n",
    "\n",
    "def stemmatize(row):\n",
    "    return [stemmer_E.stem(tok) for tok in row]\n",
    "\n",
    "sentiments['Stem'] = sentiments.StopWords.apply(stemmatize)\n",
    "sentiments.Stem.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do a sentiment analysis on each sentense and then compute a socre for each country\n",
    "\n",
    "We will compare different module:\n",
    "    - nltk.sentiment.util\n",
    "    - nltk.sentiment.vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "def sentiment_analysis(row):\n",
    "    score = sid.polarity_scores(' '.join(row))\n",
    "    return pd.Series({'pos': score['pos'], 'neg': score['neg'] })\n",
    "\n",
    "sentiments = pd.concat([sentiments, sentiments.Stem.apply(sentiment_analysis)], axis=1)\n",
    "sentiments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved the result in csv since it take quite some time to compute the score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiments.to_csv('mailScore.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aggregate by countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiments = pd.read_csv('mailScore.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggScoreByCountry(country):\n",
    "    bool = sentiments.apply(lambda x: country.isin(x.SubjectBody.split()).any(), axis=1)\n",
    "    sent = sentiments[bool]\n",
    "    if sent.empty:\n",
    "        return 0\n",
    "    print((np.mean(sent.pos) - np.mean(sent.neg))/2)\n",
    "    return (np.mean(sent.pos) - np.mean(sent.neg))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countries['Score'] = countries.apply(aggScoreByCountry, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all country that have a score of 0 (either they never appear in the mails of they have a neutral sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries = countries[countries.Score != 0]\n",
    "len(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_sorted = countries.sort(columns=['Score'])\n",
    "f, axs = plt.subplots(1,1,figsize=(15,5))\n",
    "index = np.arange(len(countries_sorted))\n",
    "bar_width = 0.95\n",
    "axs.bar(range(len(countries_sorted)), countries_sorted.Score,width=bar_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_sorted.Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
